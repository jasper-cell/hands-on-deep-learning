{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从零开始实现批量归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(is_training,X,gamma,beta,moving_mean,moving_var,\n",
    "              eps,momentum):\n",
    "    #判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        #如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X-moving_mean)/torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape) == 2:\n",
    "            #使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean)**2).mean(dim = 0)\n",
    "        else:\n",
    "            #使用二维卷积层的情况,计算通道维上（axis=1）的均值和方差。\n",
    "            #这里我们需要保持X的形状以方便后面可以做广播运算\n",
    "            mean = X.mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "        #训练模式下用当前的均方值和方差做标准化\n",
    "        X_hat = (X-mean)/torch.sqrt(var+eps)\n",
    "        #更新移动平均的均值和方差\n",
    "        moving_mean = momentum*moving_mean + (1.0-momentum)*mean\n",
    "        moving_var = momentum*moving_var + (1.0-momentum)*var\n",
    "    Y = gamma*X_hat+beta #拉伸和偏移\n",
    "    return Y,moving_mean,moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features,num_dims):\n",
    "        super(BatchNorm,self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1,num_features)\n",
    "        else:\n",
    "            shape = (1,num_features,1,1)\n",
    "        #参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        #不参与求梯度和迭代的变量，全在内存上初始化为0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "    def forward(self,X):\n",
    "        #如果x不在内存上，将moving_mean和moving_var复制到X所在的显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        #保存更新过的moving_mean和moving_var,Module实例的training属性默认为true\n",
    "        #调用.eval()后设成false\n",
    "        Y,self.moving_mean,self.moving_var = batch_norm(self.training,X,self.gamma,self.beta,self.moving_mean,\n",
    "                                                       self.moving_var,eps = 1e-5,momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用批量归一化层的Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1,6,5),\n",
    "                    BatchNorm(6,num_dims=4),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.MaxPool2d(2,2),\n",
    "                    nn.Conv2d(6,16,5),\n",
    "                    BatchNorm(16,num_dims=4),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.MaxPool2d(2,2),\n",
    "                    d2l.FlattenLayer(),\n",
    "                    nn.Linear(16*4*4,120),\n",
    "                    BatchNorm(120,num_dims =2),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Linear(120,84),\n",
    "                    BatchNorm(84,num_dims=2),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Linear(84,10)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.9818, train acc 0.788, test acc 0.804, time 44.6 sec\n",
      "epoch 2, loss 0.4490, train acc 0.866, test acc 0.847, time 43.5 sec\n",
      "epoch 3, loss 0.3622, train acc 0.879, test acc 0.840, time 51.9 sec\n",
      "epoch 4, loss 0.3243, train acc 0.890, test acc 0.873, time 44.4 sec\n",
      "epoch 5, loss 0.3049, train acc 0.893, test acc 0.864, time 43.4 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size = batch_size)\n",
    "\n",
    "lr,num_epochs = 0.001,5\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1,6,5),\n",
    "                   nn.BatchNorm2d(6),\n",
    "                   nn.Sigmoid(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(6,16,5),\n",
    "                   nn.BatchNorm2d(16),\n",
    "                   nn.Sigmoid(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   d2l.FlattenLayer(),\n",
    "                   nn.Linear(16*4*4,120),\n",
    "                   nn.BatchNorm1d(120),\n",
    "                   nn.Sigmoid(),\n",
    "                   nn.Linear(120,84),\n",
    "                   nn.BatchNorm1d(84),\n",
    "                   nn.Sigmoid(),\n",
    "                   nn.Linear(84,10)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.9933, train acc 0.791, test acc 0.836, time 42.9 sec\n",
      "epoch 2, loss 0.4585, train acc 0.863, test acc 0.836, time 50.4 sec\n",
      "epoch 3, loss 0.3674, train acc 0.877, test acc 0.866, time 41.0 sec\n",
      "epoch 4, loss 0.3303, train acc 0.886, test acc 0.835, time 40.8 sec\n",
      "epoch 5, loss 0.3063, train acc 0.893, test acc 0.877, time 41.0 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size = batch_size)\n",
    "\n",
    "lr,num_epochs = 0.001,5\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更加稳定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对全连接层和卷积层做批量归一化的方法稍有不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch提供了BatchNorm类方便操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
